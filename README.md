# AmbedkarGPT â€“ SemRAG-based RAG System

A **Retrieval-Augmented Generation (RAG)** system built by strictly following the **SemRAG (Semantic Knowledge-Augmented RAG)** research paper. This project answers questions about **Dr. B. R. Ambedkarâ€™s works** using semantic chunking, knowledge graphs, and local LLMs.

This repository is prepared as part of the **AI Engineering Intern â€“ Technical Assignment** and is fully runnable **locally for live demonstration**.

---

## ğŸš€ Features

* âœ… Semantic Chunking using **cosine similarity** (Algorithm 1 â€“ SemRAG)
* âœ… Buffer merging to preserve contextual continuity
* âœ… Token-aware chunking (1024 max tokens, 128 overlap)
* âœ… Knowledge Graph construction (entities + relationships)
* âœ… Community detection (Louvain / Leiden)
* âœ… Local GraphRAG Search (Equation 4)
* âœ… Global GraphRAG Search (Equation 5)
* âœ… Local LLM integration (Llama3 / Mistral via Ollama)
* âœ… End-to-end Q&A pipeline with citations

---

## ğŸ§  Architecture Overview (SemRAG)

```text
PDF â†’ Semantic Chunking â†’ Entity & Relation Extraction
   â†’ Knowledge Graph â†’ Community Detection
   â†’ Local Search (Eq. 4) + Global Search (Eq. 5)
   â†’ Prompt Construction â†’ Local LLM â†’ Answer
```

The implementation closely follows **Sections 3.2.1 â€“ 3.2.3** of the SemRAG paper.

---

## ğŸ“ Project Structure

```text
ambedkargpt/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Ambedkar_works.pdf
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ chunks.json
â”‚       â””â”€â”€ knowledge_graph.pkl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chunking/
â”‚   â”‚   â”œâ”€â”€ semantic_chunker.py    # Algorithm 1
â”‚   â”‚   â””â”€â”€ buffer_merger.py
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py
â”‚   â”‚   â”œâ”€â”€ graph_builder.py
â”‚   â”‚   â”œâ”€â”€ community_detector.py
â”‚   â”‚   â””â”€â”€ summarizer.py
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ local_search.py        # Equation 4
â”‚   â”‚   â”œâ”€â”€ global_search.py       # Equation 5
â”‚   â”‚   â””â”€â”€ ranker.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â”‚   â””â”€â”€ answer_generator.py
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ ambedkargpt.py         # Main pipeline
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_chunking.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tech Stack

* **Python** 3.9+
* **sentence-transformers** (all-MiniLM-L6-v2)
* **spaCy** (NER + dependency parsing)
* **networkx** (knowledge graph)
* **python-louvain / leidenalg** (community detection)
* **Ollama** (local LLM runtime)
* **Llama3-8B / Mistral-7B** (local inference)

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone <private-repo-url>
cd ambedkargpt
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download spaCy Model

```bash
python -m spacy download en_core_web_sm
```

### 5ï¸âƒ£ Setup Local LLM (Ollama)

Install Ollama: [https://ollama.ai](https://ollama.ai)

Pull model:

```bash
ollama pull llama3:8b
# or
ollama pull mistral:7b
```

---

## â–¶ï¸ Running the System

### End-to-End Pipeline

```bash
python src/pipeline/ambedkargpt.py
```

Example query:

```text
What were Dr. B. R. Ambedkar's views on social justice?
```

---

## ğŸ” Retrieval Methods Implemented

### ğŸ”¹ Local GraphRAG Search (Equation 4)

* Matches query â†’ entities â†’ related chunks
* Filters using similarity thresholds Ï„â‚‘ and Ï„ğ’¹

### ğŸ”¹ Global GraphRAG Search (Equation 5)

* Retrieves top-K community summaries
* Scores sub-points within communities

Both methods are combined during answer generation.

---

## ğŸ§ª Testing

Run all tests:

```bash
pytest tests/
```

---

## ğŸ“Œ Configuration

Key parameters are configurable in `config.yaml`:

* Buffer size
* Similarity thresholds
* Top-K retrieval limits
* LLM model selection

---

## ğŸ§¾ Notes for Live Interview Demo

* System runs **fully offline**
* No external APIs used
* Knowledge graph is built locally
* LLM inference is local (Ollama)

Demo flow:

1. Load PDF
2. Perform semantic chunking
3. Build knowledge graph
4. Ask 3â€“5 questions
5. Show retrieved context + answer

---

## ğŸ“š Reference

* **SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering**
* Zhong et al., 2025

---

## ğŸ‘¤ Author

**Prajwal Patil**
B.E. Computer Science and Engineering
AI Engineering Intern Candidate
